{"metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.11", "language": "python"}, "language_info": {"name": "python", "version": "3.11.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat_minor": 4, "nbformat": 4, "cells": [{"cell_type": "code", "source": "import os\nimport json\nfrom datetime import datetime\nfrom transformers import pipeline\nimport requests\n\nGITHUB_TOKEN = os.getenv(\"GITHUB_TOKEN\")\nREPO_NAME = os.getenv(\"GITHUB_REPO\")  \n\n# GitHub API URL (Exclude PRs by filtering on pull_request field)\nGITHUB_API_URL = f\"https://api.github.com/repos/{REPO_NAME}/issues?state=all&per_page=100&page={{}}\"\nHEADERS = {\"Authorization\": f\"token {GITHUB_TOKEN}\"}\n\n# NLP Model for issue embedding\nnlp_model = pipeline(\"feature-extraction\", model=\"sentence-transformers/all-MiniLM-L6-v2\")\n\nEXPERTISE_FILE = \"expertise_data.json\"\n\ndef generate_embedding(text):\n    \"\"\"Generate an embedding for issue text.\"\"\"\n    return nlp_model(text)[0][:4]  # Limiting to 4 dimensions for demo\n\ndef fetch_issues():\n    \"\"\"Fetch open and closed issues from GitHub API, excluding pull requests, and handling pagination.\"\"\"\n    expertise_data = {\"contributors\": {}}\n    page = 1\n    while True:\n        response = requests.get(GITHUB_API_URL.format(page), headers=HEADERS)\n        issues = response.json()\n        if not issues:\n            break  # Exit loop if no more issues\n        \n        for issue in issues:\n            if \"pull_request\" in issue:\n                continue  # Skip pull requests\n            \n            if issue.get(\"assignee\"):\n                contributor = issue[\"assignee\"][\"login\"]\n                issue_text = issue[\"title\"] + \" \" + issue.get(\"body\", \"\")\n                embedding = generate_embedding(issue_text)\n                timestamp = issue.get(\"closed_at\", datetime.utcnow().isoformat())\n                \n                if contributor not in expertise_data[\"contributors\"]:\n                    expertise_data[\"contributors\"][contributor] = {\"issues\": []}\n                \n                expertise_data[\"contributors\"][contributor][\"issues\"].append({\n                    \"title\": issue[\"title\"],\n                    \"embedding\": embedding,\n                    \"timestamp\": timestamp\n                })\n        \n        page += 1  # Move to the next page\n    \n    return expertise_data\n\ndef save_expertise_data(expertise_data):\n    \"\"\"Save expertise data to JSON file.\"\"\"\n    with open(EXPERTISE_FILE, \"w\") as f:\n        json.dump(expertise_data, f, indent=4)\n\ndef main():\n    expertise_data = fetch_issues()\n    save_expertise_data(expertise_data)\n    print(f\"Expertise data saved to {EXPERTISE_FILE}\")\n\nif __name__ == \"__main__\":\n    main()", "metadata": {"id": "4243590d-3eee-43cc-8f5d-a699cdac94f5"}, "outputs": [], "execution_count": 1}]}