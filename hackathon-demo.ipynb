{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "5c4bc00f-1766-4d24-afe1-df7ac4bcedfe",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install crewai\n",
    "!pip install crewai.tools\n",
    "!pip install transformers\n",
    "!pip install torch\n",
    "!pip install ibm-cos-sdk\n",
    "!pip install botocore\n",
    "!pip install gpt4all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e2009950-7467-40a2-969c-b2326e521bd8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import requests\n",
    "import ibm_boto3\n",
    "from botocore.client import Config\n",
    "from crewai import Agent, Task, Crew, LLM\n",
    "from crewai.tools import BaseTool\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "\n",
    "GITHUB_TOKEN = os.environ[\"GITHUB_TOKEN\"]  # Replace with a valid GitHub token\n",
    "REPO_OWNER = os.environ[\"REPO_OWNER\"]  # Replace with the repository owner\n",
    "REPO_NAME = os.environ[\"REPO_NAME\"]  # Replace with the repository name\n",
    "HEADERS = {\"Authorization\": f\"token {GITHUB_TOKEN}\"}\n",
    "# IBM COS Credentials\n",
    "COS_API_KEY_ID = os.environ[\"COS_API_KEY_ID\"]\n",
    "COS_AUTH_ENDPOINT = \"https://iam.cloud.ibm.com/identity/token\"\n",
    "COS_RESOURCE_ENDPOINT = 'https://s3.direct.us-south.cloud-object-storage.appdomain.cloud'\n",
    "BUCKET_NAME = os.environ[\"BUCKET_NAME\"]\n",
    "OBJECT_KEY = 'expertise_data.json'\n",
    "\n",
    "# Initialize IBM COS Client\n",
    "cos_client = ibm_boto3.client(\n",
    "    service_name='s3',\n",
    "    ibm_api_key_id=COS_API_KEY_ID,\n",
    "    ibm_auth_endpoint=COS_AUTH_ENDPOINT,\n",
    "    config=Config(signature_version='oauth'),\n",
    "    endpoint_url=COS_RESOURCE_ENDPOINT\n",
    ")\n",
    "\n",
    "def load_expertise_data():\n",
    "    try:\n",
    "        response = cos_client.get_object(Bucket=BUCKET_NAME, Key=OBJECT_KEY)\n",
    "        data = json.loads(response['Body'].read().decode('utf-8'))\n",
    "        print(\"Successfully loaded expertise data.\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading expertise data: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load expertise data\n",
    "EXPERTISE_FILE = load_expertise_data()\n",
    "\n",
    "llm = LLM(\n",
    "   model=\"watsonx/meta-llama/llama-3-1-70b-instruct\",\n",
    "   api_key=os.environ[\"IBM_CLOUD_API_KEY\"],\n",
    "   temperature=0.7,    # Adjust based on task\n",
    "   max_tokens=4096,    # Set based on output needs\n",
    ")\n",
    "\n",
    "\n",
    "# Load tokenizer and model for embedding generation\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "EMBEDDING_DIM = 384  # Ensure embeddings are always this size\n",
    "\n",
    "def generate_embedding(text):\n",
    "    \"\"\"Generate an embedding for the given text using AutoModel.\"\"\"\n",
    "    if not text:\n",
    "        return np.zeros(EMBEDDING_DIM).tolist()  # Return zero vector if text is None\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "    return embedding.tolist()\n",
    "\n",
    "\n",
    "# # Custom Tool to Generate Embeddings\n",
    "class EmbeddingTool(BaseTool):\n",
    "    name: str = \"Embedding Tool\"\n",
    "    description: str = \"Generates embeddings for a given text using transformers AutoModel.\"\n",
    "\n",
    "    def _run(self, specific_text: str) -> list:\n",
    "        \"\"\"Generate embeddings for the given text.\"\"\"\n",
    "        return generate_embedding(specific_text)\n",
    "\n",
    "# Agent that Executes the Tool\n",
    "embedding_agent = Agent(\n",
    "    role=\"Embedding Generator\",\n",
    "    goal=\"Generate embeddings for a given string - {specific_text}\",\n",
    "    backstory=\"An AI agent that generates embeddings using transformers.\",\n",
    "    verbose=True,\n",
    "    tools=[EmbeddingTool()],\n",
    "    llm=llm,\n",
    ")\n",
    "\n",
    "# Task with Specific Input String\n",
    "#specific_text = \"agent assignment policy faliure with invalid result\" + \" \" + \"Curently agent assignment policy is failed\"\n",
    "\n",
    "embedding_task = Task(\n",
    "    description='Generate embeddings for a given string {specific_text} pass them to another agent.',\n",
    "    expected_output=\"A list of embeddings.\",\n",
    "    agent=embedding_agent,\n",
    "    tools=[EmbeddingTool()],\n",
    "    #input=specific_text\n",
    ")\n",
    "\n",
    "# Contributor Suggestion Tool\n",
    "class ContributorSuggestionTool(BaseTool):\n",
    "    name: str = \"Contributor Suggestion Tool\"\n",
    "    description: str = \"Suggests the best contributor based on issue embedding similarity.\"\n",
    "\n",
    "    def _run(self, issue_embedding: list) -> str:\n",
    "        \"\"\"Suggest the best contributor based on cosine similarity.\"\"\"\n",
    "        if not EXPERTISE_FILE:\n",
    "            return \"No expertise data available.\"\n",
    "        \n",
    "        best_match = None\n",
    "        best_score = float(\"-inf\")\n",
    "\n",
    "        issue_embedding_tensor = torch.tensor(issue_embedding, dtype=torch.float32)\n",
    "        \n",
    "        for contributor, data in EXPERTISE_FILE.get(\"contributors\", {}).items():\n",
    "            past_embeddings = [torch.tensor(past_issue[\"embedding\"], dtype=torch.float32) for past_issue in data.get(\"issues\", [])]\n",
    "            \n",
    "            if not past_embeddings:\n",
    "                continue\n",
    "            \n",
    "            past_embeddings_tensor = torch.stack(past_embeddings)\n",
    "            similarities = torch.nn.functional.cosine_similarity(issue_embedding_tensor.unsqueeze(0), past_embeddings_tensor, dim=1)\n",
    "            max_similarity = similarities.max().item()\n",
    "            \n",
    "            if max_similarity > best_score:\n",
    "                best_score = max_similarity\n",
    "                best_match = contributor\n",
    "\n",
    "        return best_match if best_match else \"No suitable contributor found.\"\n",
    "\n",
    "# Agent for Contributor Suggestion\n",
    "suggestion_agent = Agent(\n",
    "    role=\"Contributor Matcher\",\n",
    "    goal=\"Identify the best contributor for a given issue.\",\n",
    "    backstory=\"An AI agent that suggests the best contributor based on issue embeddings.\",\n",
    "    verbose=True,\n",
    "    tools=[ContributorSuggestionTool()],\n",
    "    llm=llm,\n",
    ")\n",
    "\n",
    "# Task for Contributor Suggestion\n",
    "suggestion_task = Task(\n",
    "    description=\"Identify the best contributor for an issue based on embeddings\",\n",
    "    expected_output=\"The best matching contributor.\",\n",
    "    agent=suggestion_agent,\n",
    "    tools=[ContributorSuggestionTool()],\n",
    "    input={\"issue_embedding\": embedding_task}\n",
    ")\n",
    "\n",
    "# Comment on GitHub Issue\n",
    "def comment_on_issue(issue_number, comment):\n",
    "    url = f\"https://api.github.ibm.com/repos/{REPO_OWNER}/{REPO_NAME}/issues/{issue_number}/comments\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"token {GITHUB_TOKEN}\",\n",
    "        \"Accept\": \"application/vnd.github.v3+json\"\n",
    "    }\n",
    "    data = {\"body\": comment}\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    if response.status_code == 201:\n",
    "        print(f\"Commented on issue #{issue_number}\")\n",
    "    else:\n",
    "        print(f\"Failed to comment on issue #{issue_number}: {response.text}\")\n",
    "\n",
    "\n",
    "# Fetch unassigned GitHub issues\n",
    "def fetch_unassigned_issues():\n",
    "    url = f\"https://api.github.ibm.com/repos/{REPO_OWNER}/{REPO_NAME}/issues\"\n",
    "    headers = {\"Authorization\": f\"token {GITHUB_TOKEN}\"}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(\"Failed to fetch issues:\", response.text)\n",
    "        return []\n",
    "    \n",
    "    issues = response.json()\n",
    "    return [issue for issue in issues if not issue.get(\"assignee\")]\n",
    "\n",
    "# Process each unassigned issue\n",
    "def process_issues():\n",
    "    unassigned_issues = fetch_unassigned_issues()\n",
    "    \n",
    "    if not unassigned_issues:\n",
    "        print(\"No unassigned issues found.\")\n",
    "        return\n",
    "    \n",
    "    crew = Crew(agents=[embedding_agent, suggestion_agent], tasks=[embedding_task, suggestion_task])\n",
    "    \n",
    "    for issue in unassigned_issues:\n",
    "        if \"pull_request\" in issue:\n",
    "            continue\n",
    "        issue_text = issue[\"title\"] + \" \" + issue.get(\"body\", \"\")\n",
    "        result = crew.kickoff(inputs={\"specific_text\": issue_text})\n",
    "        print(f\"Issue: {issue['title']} -> Suggested Contributor: {result}\")\n",
    "\n",
    "        if result and result != \"No suitable contributor found.\" and result != \"No expertise data available.\":\n",
    "            comment = f\"@{result}, you seem to be the best fit for this issue. Could you take a look?\"\n",
    "            comment_on_issue(issue[\"number\"], comment)\n",
    "\n",
    "# Run the process\n",
    "process_issues()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2883f1b1-3090-4f70-9293-2eca2f27ba15"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
